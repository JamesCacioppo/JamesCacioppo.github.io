[{"body":" git stash -h - get help git stash git stash -u - also stashes untracked changes git stash -a - also stashes ignored files git stash save \"message\" - add annotation to stash git stash list - show all stashes git stash show stash@{0} - Show changed files in a stash git stash show -p - Shows diff git stash show -v stash@{0} - Show actual changes git stash pop - empty the stash and apply changes git stash apply - apply changes and keep them in your stash. Useful if you want to apply changes to multiple branches. git stash clear - delete all stashes git stash drop stash@{0} - delete a single stash  ","categories":["Docs"],"description":"Working with the Working Tree and how to stash. This page is a WIP\n","excerpt":"Working with the Working Tree and how to stash. This page is a WIP\n","ref":"/docs/git/working_tree/","tags":["git","git-stash","working tree"],"title":"The Working Tree and Stashing"},{"body":"","categories":["Docs"],"description":"A scratch pad for random notes\n","excerpt":"A scratch pad for random notes\n","ref":"/docs/randomtobesortednotes/","tags":"","title":"Random Notes"},{"body":" The following post was sourced from Tom Van Eyck’s blog over at https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/. His last post was in 2018 and his github has been idle for as long so I’ve become worried that something might happen to his blog. Because I refer to this article so frequently, I have decided to preserve it here.\n Often times developers go about writing bash scripts the same as writing code in a higher-level language. This is a big mistake as higher-level languages offer safeguards that are not present in bash scripts by default. For example, a Ruby script will throw an error when trying to read from an uninitialized variable, whereas a bash script won’t. In this article, we’ll look at how we can improve on this.\nThe bash shell comes with several builtin commands for modifying the behavior of the shell itself. We are particularly interested in the set builtin, as this command has several options that will help us write safer scripts. I hope to convince you that it’s a really good idea to add set -euxo pipefail to the beginning of all your future bash scripts.\nset -e The -e option will cause a bash script to exit immediately when a command fails. This is generally a vast improvement upon the default behavior where the script just ignores the failing command and continues with the next line. This option is also smart enough to not react on failing commands that are part of conditional statements. Moreover, you can append a command with || true for those rare cases where you don’t want a failing command to trigger an immediate exit.\nBefore #!/bin/bash  # 'foo' is a non-existing command foo echo \"bar\" # output # ------ # line 4: foo: command not found # bar # # Note how the script didn't exit when the foo command could not be found. # Instead it continued on and echoed 'bar'. After #!/bin/bash set -e # 'foo' is a non-existing command foo echo \"bar\" # output # ------ # line 5: foo: command not found # # This time around the script exited immediately when the foo command wasn't found. # Such behavior is much more in line with that of higher-level languages. Any command returning a non-zero exit code will cause an immediate exit #!/bin/bash set -e # 'ls' is an existing command, but giving it a nonsensical param will cause # it to exit with exit code 1 $(ls foobar) echo \"bar\" # output # ------ # ls: foobar: No such file or directory # # I'm putting this in here to illustrate that it's not just non-existing commands # that will cause an immediate exit. Preventing an immediate exit #!/bin/bash set -e foo || true $(ls foobar) || true echo \"bar\" # output # ------ # line 4: foo: command not found # ls: foobar: No such file or directory # bar # # Sometimes we want to ensure that, even when 'set -e' is used, the failure of # a particular command does not cause an immediate exit. We can use '|| true' for this. Failing commands in a conditional statement will not cause an immediate exit #!/bin/bash set -e # we make 'ls' exit with exit code 1 by giving it a nonsensical param if ls foobar; then echo \"foo\" else echo \"bar\" fi # output # ------ # ls: foobar: No such file or directory # bar # # Note that 'ls foobar' did not cause an immediate exit despite exiting with # exit code 1. This is because the command was evaluated as part of a # conditional statement. That’s all for set -e. However, set -e by itself is far from enough. We can further improve upon the behavior created by set -e by combining it with set -o pipefail. Let’s have a look at that next.\nset -o pipefail The bash shell normally only looks at the exit code of the last command of a pipeline. This behavior is not ideal as it causes the -e option to only be able to act on the exit code of a pipeline’s last command. This is where -o pipefail comes in. This particular option sets the exit code of a pipeline to that of the rightmost command to exit with a non-zero status, or to zero if all commands of the pipeline exit successfully.\nBefore #!/bin/bash set -e # 'foo' is a non-existing command foo | echo \"a\" echo \"bar\" # output # ------ # a # line 5: foo: command not found # bar # # Note how the non-existing foo command does not cause an immediate exit, as # it's non-zero exit code is ignored by piping it with '| echo \"a\"'. After #!/bin/bash set -eo pipefail # 'foo' is a non-existing command foo | echo \"a\" echo \"bar\" # output # ------ # a # line 5: foo: command not found # # This time around the non-existing foo command causes an immediate exit, as # '-o pipefail' will prevent piping from causing non-zero exit codes to be ignored. This section hopefully made it clear that -o pipefail provides an important improvement upon just using -e by itself. However, as we shall see in the next section, we can still do more to make our scripts behave like higher-level languages.\nset -u This option causes the bash shell to treat unset variables as an error and exit immediately. Unset variables are a common cause of bugs in shell scripts, so having unset variables cause an immediate exit is often highly desirable behavior.\nBefore #!/bin/bash set -eo pipefail echo $a echo \"bar\" # output # ------ # # bar # # The default behavior will not cause unset variables to trigger an immediate exit. # In this particular example, echoing the non-existing $a variable will just cause # an empty line to be printed. After #!/bin/bash set -euo pipefail echo \"$a\" echo \"bar\" # output # ------ # line 5: a: unbound variable # # Notice how 'bar' no longer gets printed. We can clearly see that '-u' did indeed # cause an immediate exit upon encountering an unset variable. Dealing with ${a:-b} variable assignments Sometimes you’ll want to use a ${a:-b} variable assignment to ensure a variable is assigned a default value of b when a is either empty or undefined. The -u option is smart enough to not cause an immediate exit in such a scenario.\n#!/bin/bash set -euo pipefail DEFAULT=5 RESULT=${VAR:-$DEFAULT} echo \"$RESULT\" # output # ------ # 5 # # Even though VAR was not defined, the '-u' option realizes there's no need to cause # an immediate exit in this scenario as a default value has been provided. Using conditional statements that check if variables are set Sometimes you want your script to not immediately exit when an unset variable is encountered. A common example is checking for a variable’s existence inside an if statement.\n#!/bin/bash set -euo pipefail if [ -z \"${MY_VAR:-}\" ]; then echo \"MY_VAR was not set\" fi # output # ------ # MY_VAR was not set # # In this scenario we don't want our program to exit when the unset MY_VAR variable # is evaluated. We can prevent such an exit by using the same syntax as we did in the # previous example, but this time around we specify no default value. This section has brought us a lot closer to making our bash shell behave like higher-level languages. While -euo pipefail is great for the early detection of all kinds of problems, sometimes it won’t be enough. This is why in the next section we’ll look at an option that will help us figure out those really tricky bugs that you encounter every once in a while.\nset -x The -x option causes bash to print each command before executing it. This can be a great help when trying to debug a bash script failure. Note that arguments get expanded before a command gets printed, which will cause our logs to contain the actual argument values that were present at the time of execution!\n#!/bin/bash set -euxo pipefail a=5 echo $a echo \"bar\" # output # ------ # + a=5 # + echo 5 # 5 # + echo bar # bar That’s it for the -x option. It’s pretty straightforward, but can be a great help for debugging. Next up, we’ll look at an option I had never heard of before that was suggested by a reader of this blog.\nReader suggestion: set -E Traps are pieces of code that fire when a bash script catches certain signals. Aside from the usual signals (e.g. SIGINT, SIGTERM, …), traps can also be used to catch special bash signals like EXIT, DEBUG, RETURN, and ERR. However, reader Kevin Gibbs pointed out that using -e without -E will cause an ERR trap to not fire in certain scenarios.\nBefore #!/bin/bash set -euo pipefail trap \"echo ERR trap fired!\" ERR myfunc() { # 'foo' is a non-existing command foo } myfunc echo \"bar\" # output # ------ # line 9: foo: command not found # # Notice that while '-e' did indeed cause an immediate exit upon trying to execute # the non-existing foo command, it did not case the ERR trap to be fired. After #!/bin/bash set -Eeuo pipefail trap \"echo ERR trap fired!\" ERR myfunc() { # 'foo' is a non-existing command foo } myfunc echo \"bar\" # output # ------ # line 9: foo: command not found # ERR trap fired! # # Not only do we still have an immediate exit, we can also clearly see that the # ERR trap was actually fired now. The documentation states that -E needs to be set if we want the ERR trap to be inherited by shell functions, command substitutions, and commands that are executed in a subshell environment. The ERR trap is normally not inherited in such cases.\nConclusion I hope this post showed you why using set -euxo pipefail (or set -Eeuxo pipefail) is such a good idea. If you have any other options you want to suggest, then please let me know and I’ll be happy to add them to this list.\n","categories":["Docs"],"description":"Safer bash scripts, taken from Tom Van Eyck\n","excerpt":"Safer bash scripts, taken from Tom Van Eyck\n","ref":"/docs/randomtobesortednotes/safer_bash_scripts_with_set_euxo_pipefail/","tags":["bash","scripts"],"title":"Safer bash scripts with 'set -euxo pipefail'"},{"body":"jq '.prefixes[] | select(.region==\"us-gov-east-1\") | select(.service==\"EC2\")' https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html\n","categories":"","description":"A scratch pad for random notes\n","excerpt":"A scratch pad for random notes\n","ref":"/docs/randomtobesortednotes/scratch_pad/","tags":"","title":"Scratch Pad"},{"body":"","categories":["Docs"],"description":"How to use Pulumi. This is a WIP.\n","excerpt":"How to use Pulumi. This is a WIP.\n","ref":"/docs/randomtobesortednotes/pulumi/","tags":["iac","pulumi"],"title":"Pulumi"},{"body":"Certificate types X509 PEM PEM (originally “Privacy Enhanced Mail”) is the most common format for X.509 certificates, CSRs, and cryptographic keys. A PEM file is a text file containing one or more items in Base64 ASCII encoding, each with plain-text headers and footers (e.g. -----BEGIN CERTIFICATE----- and -----END CERTIFICATE-----). A single PEM file could contain an end-entity certificate, a private key, or multiple certificates forming a complete chain of trust. (Source: SSL.com)\nCommon file extensions are .crt, .cer, .pem, .key, ca-bundle.\nView contents of the certificate in CERTIFICATE_FILE: openssl x509 -in CERTIFICATE_FILE -text -noout\nConvert PEM to DER: openssl x509 -outform der -in PEM_FILE -out DER_FILE\nConvert PEM to PKCS#7: openssl crl2pkcs7 -nocrl -certfile CERTIFICATE_PEM_FILE -certfile CA_CHAIN_PEM_FILE -out OUTPUT_FILE\nConvert PEM to PKCS#12: openssl pkcs12 -export -out CERTIFICATE_FILE -inkey PRIVATE_KEY_FILE -in CERTIFICATE -certfile CA_CHAIN\nDER DER (Distinguished Encoding Rules) is a binary encoding for X.509 certificates and private keys. Unlike PEM, DER-encoded files do not contain plain text statements such as -----BEGIN CERTIFICATE-----. DER files are most commonly seen in Java contexts. (Source: SSL.com)\nCommon file extensions are .der and .cer.\nView contents of CERTIFICATE_DER_FILE: openssl x509 -inform der -in CERTIFICATE_DER_FILE -text -noout\nConvert CERTIFICATE_DER_FILE to a PEM: openssl x509 -inform der -in CERTIFICATE_DER_FILE -out CERTIFICATE_PEM_FILE\nCertificate container formats PKCS#7 PKCS#7 (also known as P7B) is a container format for digital certificates that is most often found in Windows and Java server contexts, and usually has the extension .p7b. PKCS#7 files are not used to store private keys.\nPKCS#12 PKCS#12 (also known as PKCS12 or PFX) is a common binary format for storing a certificate chain and private key in a single, encryptable file, and usually have the filename extensions .p12 or .pfx.\nConvert PKCS to x509 Extract the private key:\nopenssl pkcs12 -in P12_CERT_FILE -out X509_KEY_FILE -nocerts -nodes Extract the certificate:\nopenssl pkcs12 -in P12_CERT_FILE -out X509_CERT_FILE -nokeys -nodes Extract the CA chain:\nopenssl pkcs12 -in P12_CERT_FILE -out X509_CA_CHAIN_FILE -nokeys -cacerts -chain Handy commands Misc commands Get an x509 secret from Kubernetes and output details:\nkubectl get secret SECRET_NAME -ojson | jq -r '.data.\"KEY\"' | base64 -d | openssl x509 -text Create a self signed X.509 certificate:\nopenssl req -x509 -nodes -newkey rsa:4096 -keyout \"PRIVATE_KEY_FILE\" -out \"PUBLIC_KEY_FILE\" -subj \"SUBJECT\" Extract the client key and pass to AWK to remove all newline characters:\nopenssl pkcs12 -in PKCS12_FILE -clcerts -nokeys -password pass:PASSWORD | awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}'    option arg explanation     -in file Input file to read from. STDIN if not provided   -clcerts  Only output client certificates (not CA certs)   -nokeys  Do not output private keys   -password  not found in linux or mac openssl   pass:password  Where password is the password    ","categories":["Docs"],"description":"OpenSSL command cheat sheet\n","excerpt":"OpenSSL command cheat sheet\n","ref":"/docs/encryption/openssl-cheat-sheet/","tags":["openssl","ssl","tls","certificates"],"title":"OpenSSL Cheat Sheet"},{"body":"Creating tags There are two types of tags, lightweight and annotated. The following explanation was taken from a Stack Overflow post:\n A lightweight tag is very much like a branch that doesn’t change - it’s just a pointer to a specific commit.\n  Annotated tags, however, are stored as full objects in the Git database. They’re checksummed; contain the tagger name, email, and date; have a tagging message; and can be signed and verified with GNU Privacy Guard (GPG). It’s generally recommended that you create annotated tags so you can have all this information; but if you want a temporary tag or for some reason don’t want to keep the other information, lightweight tags are available too.\n Create lightweight tag git tag TAG Create annotated tag git tag -a TAG -m COMMIT MESSAGE Handling tags Pushing new tags git push origin TAG Deleting local tags git tag -d TAG Deleting remote tags git push origin --delete TAG Listing tags git tag Sorting output Use the sort switch --sort=\u003ctype\u003e, where type can be:\n refname: lexicographic order version:refname or v:refname: tag names are treated as versions Reverse order Prepend type with - to reverse order.    Examples Lexical sort\n$ git tag -l --sort=refname \"foo*\" foo1.10 foo1.3 foo1.6 Version sort\n$ git tag -l --sort=version:refname \"foo*\" foo1.10 foo1.6 foo1.3 Reverse version sort\n$ git tag -l --sort=-version:refname \"foo*\" foo1.10 foo1.6 foo1.3 Reverse lexical sort\n$ git tag -l --sort=-refname \"foo*\" foo1.6 foo1.3 foo1.10 ","categories":["Docs"],"description":"How to create and handle tags in Git.\n","excerpt":"How to create and handle tags in Git.\n","ref":"/docs/git/tagging/","tags":["git","git-tag","tagging"],"title":"Git Tagging"},{"body":"This is a WIP git-add git-commit git-merge git-log  git log --oneline git log --stat git log -p git shortlog - groups commits by user git log --graph --oneline --decorate - the golden command  Additional resources: https://www.atlassian.com/git/tutorials/git-log\ngit-blame git-diff git-show Show information about various objects\ngit-restore Restore a file from a previous commit\ngit restore --source HEAD~1 FILE_NAME Restore a file from HEAD\ngit restore FILE_NAME Restore all files in the current directory from HEAD\ngit restore . git-reset git reset --soft COMMIT git reset --hard COMMIT Removing a commit Delete the last commit: git reset --hard HEAD~1 Force push the changes: git push -f remote branch\n","categories":["Docs"],"description":"Various git commands.  This is a WIP.\n","excerpt":"Various git commands.  This is a WIP.\n","ref":"/docs/git/commands/","tags":["git","git commands"],"title":"Git Commands"},{"body":"Configuration Git configuration location There are three places where configuration is stored:\n System: path/etc/gitconfig Global: ~/.gitconfig or /.config/git/config Local: repo/.git/config  When using the git config command you can pass --system, --global, or --local to specify which configuration you’d like to modify. The list above is in order of precedence from lowest to highest. Values in the local config will override values in global and system. Values in global will override values in system.\nTo show all configurations and their source execute git config --list --show-origin\nHow to use git-config To see the man page, which is really good for this command, execute man git-config. The “name” of the option to affect is the section and key separated by a period (e.g. alias.br).\nThere are several options query/set/replace/unset.\nTo create git config alias.br 'branch -a'\nTo unset git config --unset alias.br\nRequired configurations In order to commit in a repo you’ll need to ensure that two values are set at some configuration level:\n Email: git config –global user.name “USER-NAME\" Username: git config –global user.email “EMAIL\"  Aliases A very useful configuration is an alias. It allows you to create a short alias for a long command. Another benefit is having an alias is like having notes you can look up when you forget a seldom used command.\nTo create an alias git config –global alias.ALIAS-NAME COMMAND\nAn example would be to set git br to git branch -a\n$ git config --global alias.br 'branch -a' ","categories":["Docs"],"description":"How to configure git.\n","excerpt":"How to configure git.\n","ref":"/docs/git/configuration/","tags":["git","git-config"],"title":"Git Configuration"},{"body":"The first thing to understand about Git is it’s a distributed Source Control Manager (SCM). This means that the repository isn’t stored in a central location. It’s stored on every developer’s computer. When it was first created this was how it was used. We now have a feature called a remote that has become ubiquitous. A remote is commonly a Git server which can store your repository in a central place for all developers on your team to access. The first step in working with Git is almost always cloning.\nClone a repo Cloning a repo is the act of downloading a copy of the repo to your local machine so you can work on it. The git clone command will create a new sub directory within your present working directory and clone the repo to it. To clone a repo using the HTTPS protocol:\n$ git clone https://github.com/JamesCacioppo/git-zero-to-hero-demo.git To clone a repo using SSH:\n$ git clone git@github.com:JamesCacioppo/git-zero-to-hero-demo.git Remotes Display configured remotes:\n$ git remote -v origin git@github.com:JamesCacioppo/JamesCacioppo.github.io.git (fetch) origin git@github.com:JamesCacioppo/JamesCacioppo.github.io.git (push) To add a remote:\ngit remote add upstream URL To change a local branch’s upstream tracking:\ngit branch --set-upstream-to REMOTE_NAME/BRANCH_NAME $ git branch --set-upstream-to origin/main branch 'main' set up to track 'origin/main'. Branching When you first clone a repo you’ll be in the default branch. This was historically called master and is now often named main. Most organizations use a branching strategy which usually involves creating a branch, committing changes to the branch, and then merging that branch back into main.\nCreating Branches The formal way to create a branch is with the git-branch command:\ngit branch BRANCH-NAME However, the previous command does not place you on that branch and you’d still need to use the git-checkout command to switch branches. To create a branch and switch to it in one command use git-checkout:\ngit checkout -b BRANCH-NAME At this point your local repo is tracking the new branch but the remote is not. To update the remote:\ngit push --set-upstream origin BRANCH-NAME Useful Branching Commands To list local branches: git branch\nTo list branches locally and remotely: git branch -a\nTo rename the current branch: git branch -m BRANCH-NAME\nTo checkout a branch use: git checkout BRANCH-NAME\ngit-checkout The git-checkout command only moves the HEAD pointer, not the branch pointer. This is different from git-reset.  Deleting Branches When it comes time to delete a branch there are a few things to note.\n Deleting a branch locally is different from deleting a branch on the remote. There are two ways to remove a branch locally listed below.  The “safe” way will only remove the branch if its changes have been merged into main while the “forcefull” method will remove the branch regardless of its status.   You cannot remove a branch if you have it checked out. When you delete a remote branch your local repo does not know and will need to be updated. To do this you must remove the branch refs.  To safely remove a local branch use git branch -d BRANCH-NAME\nTo forcefully remove a local branch use git branch -D BRANCH-NAME\nTo remove a remote branch use git push origin –delete BRANCH-NAME\nTo remove deleted branch refs use git remote prune origin\nBranching Strategies When a team is using Git, some sort of workflow or branching strategy is required for the team to develop effectively. Junior developers need an understanding of various common strategies so they can onboard quickly to various teams and projects. Intermediate and senior developers will need a more in-depth understanding of the common strategies, their variants, benefits and drawbacks of each, and when to choose one strategy over another. What follows is just a brief introduction into a few common strategies.\nGitflow In 2010, Vincent Driessen documented Gitflow. In a vacuum of well documented and capable strategies, Gitflow became almost ubiquitous in development teams and is still almost required knowledge. While this strategy works well under certain conditions, Gitflow has many drawbacks. In fact, Vincent even updated his post in 2020 explaining that it shouldn’t necessarily be the default or go-to for all dev teams. We’ll discuss some of the pro’s and con’s of Gitflow, but first, let’s take a look at how it works.\nTwo branches live forever. They are develop and master. The develop branch is the branch from which almost all work is done. Developers will create feature branches from develop and merge their work back in.\nDevelopers will also create release branches from develop. The purpose of creating these is to start preparing code for release or production deployment. Once this prep work is complete, release branches will be merged into master.\nBug and hotfix branches will be created from master and when complete they will be merged back into master as well as develop.\nAs you can see, this is a complex strategy which can result in some interesting merge conflicts. It also does not facilitate CI/CD.\nThere are circumstances, however, where releasing and deploying at a high frequency is undesirable or impossible. In these cases, this strategy can be helpful as there’s a natural delay between selecting a release candidate and a push to production. The dedicated release branch allows developers to fix issues with the release candidate while continuing development of the baseline on develop.\nGithub flow Github flow, designed by Github, is meant to provide many of the benefits seen in Gitflow while massively reducing complexity. Their documentation is clear and concise and definitely worth a look.\nIn Github flow, there is only one long lived branch and it’s the main branch. All working branches come from main and merge back into main. This includes branches for features, ops updates, bug and hot fixes, etc. When a commit on master is chosen for release a version tag is applied to it.\nThis strategy is very flexible. Testing can be done at any point. In fact, we’ve found that it’s effective to perform builds, deployments, and testing when a PR is created and on each subsequent commit which changes a PR. Then, we perform the same and more testing when merged back into main, build and push artifacts, and deploy into UAT environments.\nTrunk One of the main issues with many common strategies is the rate of integration. Continuous Integration (CI) means integrating, or merging, code back into the mainline as frequently as possible. In Gitflow, Github flow, and other similar strategies, developers commonly keep feature branches open for days if not entire sprints. What often results is sometimes called “integration hell” or “merge hell” as developers attempt to merge their code, which has become further and further out of date from the mainline as time has passed.\nEnter Trunk Based workflows.\nThe idea is that code is integrated into the mainline as often as every commit and at least once every 24 hours. Small two dev teams who are paired programming may commit directly to master while larger “at scale” teams will need to use feature branches. The key difference here is that feature branches should be very short lived and should be integrated at least once every 24 hours.\nIn order for this to work, thorough testing must be conducted before merging using various methods such as pre-commit hooks and the baseline should be kept in good working condition. If a bug is found in the baseline fixing it should be the priority.\nA great resource is the site at ContinuousDelivery.com. The page on Continuous Integration is especially relevant to this topic.\n","categories":["Docs"],"description":"Branching concepts and commands.\n","excerpt":"Branching concepts and commands.\n","ref":"/docs/git/branching/","tags":["git","branching"],"title":"Git Branching"},{"body":"","categories":"","description":"All things encryption related.\n","excerpt":"All things encryption related.\n","ref":"/docs/encryption/","tags":"","title":"Encryption"},{"body":"","categories":"","description":"Git from zero to hero.  Basic to advanced concepts along with examples and tutorials.\n","excerpt":"Git from zero to hero.  Basic to advanced concepts along with examples …","ref":"/docs/git/","tags":"","title":"Git"},{"body":"The BLUF How to set up kube-bench scans and export reports in an EKS cluster.\nThe long description Running kube-bench scans as a k8s job against an EKS cluster is a pretty simple task. Even setting kube-bench up as a cron job is a fairly simple task. The issue arises when you have to do something with the results of the scan.\nThe answer we settled on was executing kube-bench as an init container, saving stdout to a file on a pvc, mounting the pvc to the AWS CLI container, and pushing the file to an S3 bucket. Our decisions were driven by a requirement which dictated that the reports be made available to a third party who would process them in a manner of their choosing.\nThe kube-bench tool can output files in JSON and JUNIT formats. For my purposes, the best solution is to take a JSON file and push it to an S3 bucket where it can be stored, retrieved, and fed into another system. In an ideal world you’d want to push the output directly into a system under your control. In many cases you just have to make the results available to another party.\nThe process can be broken down into two main components: configuring AWS and configuring Kubernetes resources.\nConfiguring AWS Create an S3 bucket Name it kluster-bucket\nCreate Identity provider An identity provider tied to the cluster’s OIDC provider is required to link AWS IAM to Kubernetes access control. In many cases, you may already have one in place. If not, follow these steps:\n Copy the OpenID Connect provider URL from the cluster details at EKS \u003e Clusters \u003e cluster name Go to IAM -\u003e Identity providers and click Add Provider Select OpenID Connect, enter the Provider URL copied from the cluster details in step 1, and select sts.amazonaws.com in Audience.    Create an IAM Policy Create an IAM policy allowing write access to a specific S3 bucket:\n{ “Version”: “2012–10–17”, “Statement”: [ { “Effect”: “Allow”, “Action”: [ “s3:PutObject” ], “Resource”: “arn:aws:s3:::kluster-bucket/*” } ] } Create an IAM Role  Go to IAM \u003e Roles Click Create Role Select Web identity for Trusted entity type Under Identity provider, select the OIDC Identity provider created earlier Select sts.amazonaws.com for Audience Add the S3 IAM policy created earlier Name and finalize the role. For our purposes, we’ll name the role kluster-s3-write-access  Tie the role to a specific K8s namespace and service account (optional) In the new role, edit the trust relationship. The dictionary Statement.Condition.StringEquals will have a key for the OIDC identity provider ending in aud and a value of sts.amazon.com. Update these to sub and system:serviceaccount:namespace name:service account name respectively. They should look as follows:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/1234567890AABBCCDDEEFF1234568901\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"oidc.eks.us-east-1.amazonaws.com/id/1234567890AABBCCDDEEFF1234568901:sub\": \"system:serviceaccount:saymynamespace:s3-write\" } } } ] } Configuring Kubernetes Create a K8s service account Create a service account in the kube-bench namespace. This service account ties to the AWS IAM role created earlier as follows:\napiVersion:v1kind:ServiceAccountmetadata:name:s3-writenamespace:kube-benchannotations:eks.amazonaws.com/role-arn:arn:aws:iam::123456789012:role/kluster-s3-write-accessCreate a K8s Cronjob Create a Cronjob as follows:\napiVersion:batch/v1beta1kind:CronJobmetadata:name:kube-benchnamespace:kube-benchspec:schedule:\"0 10 * * *\"jobTemplate:spec:template:spec:hostPID:truecontainers:- name:s3-pushimage:amazon/aws-cli:2.7.29command:[\"bash\",\"-c\",'aws s3 cp /report/kube-bench-report s3://kluster-bucket/kube-bench-report-$(date -u +%Y-%m-%dT%H.%M.%S)']volumeMounts:- name:empty-dirmountPath:/reportinitContainers:- name:kube-benchimage:aquasec/kube-bench:v0.6.0imagePullPolicy:IfNotPresentcommand:[\"kube-bench\",\"run\",\"--targets\",\"node\",\"--benchmark\",\"eks-1.0.1\",\"--json\",\"--outputfile\",\"/report/kube-bench-report\"]volumeMounts:- name:var-lib-kubeletmountPath:/var/lib/kubeletreadOnly:true- name:etc-systemdmountPath:/etc/systemdreadOnly:true- name:etc-kubernetesmountPath:/etc/kubernetesreadOnly:true- name:empty-dirmountPath:/reportserviceAccountName:s3-writerestartPolicy:Nevervolumes:- name:var-lib-kubelethostPath:path:\"/var/lib/kubelet\"- name:etc-systemdhostPath:path:\"/etc/systemd\"- name:etc-kuberneteshostPath:path:\"/etc/kubernetes\"- name:empty-diremptyDir:{}","categories":["Docs"],"description":"Set up kube-bench to perform scans and export the results.\n","excerpt":"Set up kube-bench to perform scans and export the results.\n","ref":"/docs/kube-bench/","tags":["k8s","kubernetes","security","docs","kube-bench"],"title":"Kube Bench"},{"body":" This is a placeholder page. Replace it with your own content.\n Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\n There should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\n General Styling This is a normal paragraph following a header.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nSecond Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n Header 3 This is a code block following a header. Header 4  This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header.  Header 5  This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header.  Header 6    What Follows     A table A header   A table A header   A table A header     There’s a horizontal rule above and below this.\n Here is an unordered list:\n Liverpool F.C. Chelsea F.C. Manchester United F.C.  And an ordered list:\n Michael Brecker Seamus Blake Branford Marsalis  And an unordered task list:\n Create a Hugo theme Add task lists to it Take a vacation  And a “mixed” task list:\n Pack bags ? Travel!  And a nested list:\n Jackson 5  Michael Tito Jackie Marlon Jermaine   TMNT  Leonardo Michelangelo Donatello Raphael    Definition lists can be used with Markdown syntax. Definition headers are bold.\n Name Godzilla Born 1952 Birthplace Japan Color Green   Tables should have bold headings and alternating shaded rows.\n   Artist Album Year     Michael Jackson Thriller 1982   Prince Purple Rain 1984   Beastie Boys License to Ill 1986    If a table is too wide, it should scroll horizontally.\n   Artist Album Year Label Awards Songs     Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life   Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain   Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill     Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\"; Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. Inline code inside table cells should still be distinguishable.\n   Language Code     Javascript var foo = \"bar\";   Ruby foo = \"bar\"{     Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nThe photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA.\nComponents Alerts  This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam eripitur? Sitim noceat signa probat quidem. Sua longis fugatis quidem genae.\nPixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic.\nContact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly.\nExternal Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat.\nThis is the final element on the page and there should be no margin below this. ","categories":["Docs"],"description":"A complete breakdown of widely accepted Markdown\n","excerpt":"A complete breakdown of widely accepted Markdown\n","ref":"/docs/markdown/","tags":["markdown","md"],"title":"Markdown"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/coffee/","tags":"","title":"Coffee Stuff"},{"body":" Here you will find the main purpose of this site: documentation, cheat sheets, and tutorials.\n ","categories":"","description":"","excerpt":" Here you will find the main purpose of this site: documentation, …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/tech-things/","tags":"","title":"Tech Things"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/docs/","tags":"","title":"Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git/","tags":"","title":"git"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git-stash/","tags":"","title":"git-stash"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/working-tree/","tags":"","title":"working tree"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/bash/","tags":"","title":"bash"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/scripts/","tags":"","title":"scripts"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/iac/","tags":"","title":"iac"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pulumi/","tags":"","title":"pulumi"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/certificates/","tags":"","title":"certificates"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openssl/","tags":"","title":"openssl"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ssl/","tags":"","title":"ssl"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tls/","tags":"","title":"tls"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git-tag/","tags":"","title":"git-tag"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tagging/","tags":"","title":"tagging"},{"body":"The BLUF If you walk into a coffee shop and see these red flags, do yourself a favor and just get a drip coffee.\nThe long version As the fall season kicks into gear I find myself stopping by coffee shops along the bike trail more often. That little mid-ride treat becomes almost essential around the 30 mile mark. Serious cyclists know the value of caffeine dosing. It’s a major reason for the caffeine content in Gu. Nothing replaces the body warming feeling of a hot cup of coffee on a cold day. Those 60+ mile rides can be as much of a mental challenge as they are physical so little pick-me-ups can make all of the difference.\nEach time I’ve stopped in a coffee shop, however, I’ve been horribly disappointed and have learned to look out for a few key indicators before choosing between an espresso based drink and a drip coffee. Save yourself some money and heart ache by watching for these red flags!\nRed flag 1: Portafilter mishandling The portafilter is a chrome plated hunk of brass or bronze. The really high end ones are solid stainless steel. Why? Because that makes it a large thermal mass. Thermal mass, or thermal capacitance provides inertia against temperature fluctuations. You see, the name of the game in espresso is consistency (generally speaking, though there are some exceptions but that’s usually more to do with pressure and flow profiling and is way outside the scope of this post). The group head is tied to the boiler, either by a thermo-siphon, or by physical proximity. Either way, this is why espresso machines take a long time to warm up, because you have to let the group head match the temp of the boiler and the group head is also a massive chunck of metal. Then, that big hunk of metal called a portafilter sits in that group head and equalizes temperature. The idea here is that the water is always the right temp from the moment it leaves the boiler all the way into the cup. This is important for consistent extraction. Leaving the portafilter on the group head keeps the area where water and coffee mix the right temperature. All of that thermal capacity helps keep everything stable.\nSo how does one “mishandle” a portafilter? Leave it out of the group head or rinse it with tap water. Ideally, the barista will remove the portalfilter, quickly prep the puck, pull a shot, knock the puck out of the portafilter, wipe it down with a towel, and place it back in the group head. If a rinse is really needed, do it with brew water out of the brew head.\nIf you walk into a coffee shop and see the portalfilter just hanging around sitting on a counter or on the drip plate, do yourself a favor and just order a drip coffee. If you see the barista do something like rinse the portafilter in the sink, skip the espresso based beverage and grab a drip coffee.\nRed flag 2: Dosing 1:2, 20 grams in and 40 grams out (18g in and 36g out for smaller portafilter baskets). That’s a great place to start for any roast and it’s where I point people when they’re starting out. What it means is you put 20 grams of ground coffee into the portafilter and you pull the shot until you have 40 grams of espresso in your cup. Consistency is key, remember? So if you stick to a 20g dose and adjust your grind so you get 40g out in 18-20s you should be in a great place.\nIn the old days they used to fill the portafilter until it was heaping over the top then level it by knocking the excess off with a swipe of their finger. Super precise. They would also base their shot purely on time. In fact, many commercial machines still have programable single and double shots based on time. Really good baristas can still make this work but the chances of getting a bad shot are much higher. The time based shot should still get it in the ballpark provided everything else is consistent, but that dosing method of old has to be let out to pasture.\nThese days, commercial grinders come with built in scales. You pop the portafilter into the holder, press it to activate the grinder, and the grinder just does its thing and shuts off at the preset weight. Even if a shop has older grinders that don’t operate by weight, external scales are cheap and plentiful these days. There’s simply no excuse for playing the old guessing game anymore.\nSo watch the barista when you walk into the next shop. If you see a doser and no scale, just ask for a drip coffee.\nRed flag 3: Puck prep When you pull that lever (or push the button) water hits that puck at a pressure of 9 bar. That’s about 130psi. There needs to be enough resistance such that it takes 20-ish seconds for 40 grams of water to pass. Moreover, you want that water to evenly saturate the whole puck and pass over all of the grinds rather than channel through it in a few specific spots. If channeling happens, the grinds in the channel will be over extracted while the rest will be under extracted.\nAgain, we find ourselves discussing consistency. The grind needs to be consistent, the distribution of grinds must be consistent, and the compactness across the puck has to be consistent. So when you walk in you should see at least two things: a burr grinder and a distribution tool. In the best case you’ll also see a nice tamper but there’s a lot of debate amongst the best baristas as to whether or not the tamper still has a place.\nWhat you don’t want to see is clumpy mounds of coffee grinds piling into a portafilter and then a barista just throwing a tamper on top and pressing away. That’s just a recipe for uneven distribution and channeling galore.\nRed flag 4: Milk steaming Milk steaming is a whole art form of itself. It’s almost as if the first art one must master is espresso and the second art is steaming milk.\nMilk is fickle. There’s a narrow, about 10 degree Fahrenheit, window where the milk takes on a natural sweetness. Any lower or higher and that sweetness is lost. It doesn’t take long to get there either. My Rocket Espresso Mozzafiato will steam a six ounce pitcher in about 18 seconds. Frothing, or texturing, should be done at the very beginning and will only take about three seconds on any machine with a dedicated steam boiler. The whole thing is a very quick process and most baristas will feel the temperature with one hand on the bottom and side of the pitcher.\nWhat I see all too often is a barista over-aerate the milk for a few seconds, then set the pitcher down and let it continue to steam while they do something else. If you see this, walk away.\n","categories":"","description":"Why becoming a home barista can ruin coffee for you.","excerpt":"Why becoming a home barista can ruin coffee for you.","ref":"/blog/2022/10/09/coffee-woes/","tags":"","title":"Coffee Woes"},{"body":"I spent Friday night and Saturday morning falling down the rabbit hole of broken docs. Every time I update one code snippet I stumble on another that’s broken. I’m not mad. I enjoy documentation. I’m broken, I know, but I really do enjoy it. There’s no better way to solidify your knowledge on a subject than to teach others. The medical field used to have a saying: “See one, do one, teach one.”\nWhether or not I’m enjoying wasting my Saturday morning fixing broken docs is irrelevant. I’m here because I got an email from a data service provider my customer has told to integrate with my application. This individual innocently and politely asked for help as they were attempting to use our example in our docs and it wasn’t working. After reviewing, I had to inform them in front of my customer that our docs are out of date and that he needed to change his commands.\nIt was a bad look and it can be embarrassing. When a dev team leaves the task of documentation for after the feature development, documentation often gets forgotten or pushed to the right as more pressing matters come up. This trend continues until one day you find that more than half of your docs are out of date.\nThe solution is to push documentation to the left! When are updated docs on a feature needed? When that feature code goes live. However, if you wait until the code goes live to publish the updated docs then you’re already behind. For this reason, we’ve implemented a rule that documentation must be completed before the story can be moved to done, the bug ticket can be resolved, and the PR can be merged. In other words, documentation is always part of the acceptance criteria. This adds extra layers of accountability as now every reviewer is also responsible for making sure that documentation is complete before approving.\nPushing documentation to the left is, like all other things, a practice of taking a little more pain up front to avoid a lot later. Give it a try!\n","categories":"","description":"Why you should push documentation to the left on your projects","excerpt":"Why you should push documentation to the left on your projects","ref":"/blog/2022/10/08/pushing-documentation-to-the-left/","tags":"","title":"Pushing Documentation to the Left"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/branching/","tags":"","title":"branching"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git-commands/","tags":"","title":"git commands"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/git-config/","tags":"","title":"git-config"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/markdown/","tags":"","title":"markdown"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/md/","tags":"","title":"md"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2476442_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2476442_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  About James Some things you might want to know about me.        Husband, father, and super nerd.\n    I'm a systems engineer with a long background in industrial control systems (ICS). I spent many years in the oil and gas industry designing, building, deploying, and supporting automation systems for what we called \"downstream\" delivery. This time was a ton of fun because one day I could be hacking away at the Linux based automation servers and the next I could be out in the field dressed in head to toe Nomex testing and fixing electrical system terminations.      I later moved into a DevOps role in the renewable energy sector. This was a dream come true. Not only had I moved into the cutting edge of cloud computing, I could still leverage my ICS background to help accomplish what I still consider to be one of humanities most important missions. It was here that I was finally exposed to concepts such as Infrastructure as Code (IaC), the value of writing IaC in an idempotent and deterministic manner, and the mac daddy of the cloud, Kubernetes.       Today, I have found myself managing a team of the highest performing engineers building software solutions on the bleeding edge of technology.      After work, I'm a family man. I try to spend as much time with my family as possible. I bike, hike, enjoy woodworking, working on cars, and photography. I'm a massive coffee nerd. I love cigars. I hunt, shoot, and fish as time permits.     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"About James"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/blog/","tags":"","title":"Docsy Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/k8s/","tags":"","title":"k8s"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kube-bench/","tags":"","title":"kube-bench"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"kubernetes"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu4fe1f5561ffcc006908db5108c0ea99f_801041_960x540_fill_q75_catmullrom_smart1.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu4fe1f5561ffcc006908db5108c0ea99f_801041_1920x1080_fill_q75_catmullrom_smart1.jpg); } }  A Modern Engineer Learn More   My Github   Notes, tutorials, and the thoughts and musings of a modern engineer. Pull a shot, poor some latte art, and let's dig in!\n         The primary purpose of this site is to provide polished versions of my notes and documentation in a way that’s accessible to everyone.\nAn ancillary purpose is to provide myself with a place to express and document everything from my thoughts on branching strategies to cigar and whisky tasting notes.\n      Coffee’s brewing! We’re now pulling shots of LoCo Beans’s Espresso Grande.\n    Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\n Read more …\n   Follow me on Twitter! Follow me on Twitter for updates!\n Read more …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Modern Engineer"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/security/","tags":"","title":"security"}]